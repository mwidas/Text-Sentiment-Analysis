---
title: "Lab 1: NYT API"
author: "Melissa Widas"
date: "2024-04-03"
output: html_document
---

## Assignment (Due Tuesday 4/9 11:59pm)
Reminder: Please suppress all long and extraneous output from your submissions (ex:  lists of tokens).

1.  Create a free New York Times account (<https://developer.nytimes.com/get-started>)

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite) #convert results from API queries into R-friendly formats 
library(tidyverse) #tidy
library(tidytext) #text data management and analysis
library(ggplot2) #plot word frequencies and publication dates

#assign API key.  When you create a NYT Dev account, you will be given a key
source(here::here("API_KEY.R"))
API_KEY <- print(API_KEY)

```

2.  Pick an interesting environmental key word(s) and use the {jsonlite} package to query the API. Pick something high profile enough and over a large enough time frame that your query yields enough articles for an interesting examination.

```{r, message=FALSE}
term1 <- "mine" 
term2 <- "environment"
begin_date <- "20150101"
end_date <- "20230101"

#construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
                  term1, "%20", term2,
                  "&begin_date=", begin_date,
                  "&end_date=", end_date,
                  "&facet_filter=true",
                  "&api-key=", API_KEY)

#run initial query
initialQuery <- fromJSON(baseurl)

maxPages <- 10
#initiate a list to hold results of our for loop
pages <- list()

#loop
for(i in 1:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch
  Sys.sleep(12) # make the system wait 12 seconds in between pulls from api -> within documentation
}

#bind the pages and create a tibble from nytDat
nyt_df <- bind_rows(pages)
```

3.  Recreate the publications per day and word frequency plots using the first paragraph field.  This time filter on the response.docs.news_desk variable to winnow out irrelevant results.

```{r article-type}
nyt_df %>% 
  group_by(response.docs.news_desk) %>%
  summarize(count=n()) %>% #This creates a new data frame with the count of records for each news_desk.
  mutate(percent = (count / sum(count))*100) %>% #add percent of total column
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.news_desk, fill=response.docs.news_desk), stat = "identity") + coord_flip()
```

```{r date-plot}
nytDat <- nyt_df

nytDat %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>% 
  group_by(pubDay) %>%
  arrange(pubDay) |> 
  summarise(count=n()) %>%
  filter(count >= 2) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") +
  coord_flip() #bring date so bars go lengthwise
```

```{r word_frequencies}
#use tidytext::unnest_tokens to put in tidy form.  
#If there are some types of news that we'd like to exclude, we can filter()
tokenized <- nytDat %>%
  filter(response.docs.news_desk!=c("Games","Sports")) %>%
unnest_tokens(word, response.docs.lead_paragraph) #word is the new column, paragraph is the source

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col()+
  labs(y = NULL)
```


-   Make some (at least 3) transformations to the corpus including: add context-specific stopword(s), stem a key term and its variants, remove numbers)

```{r stop-words, message=FALSE}
#load stop words
data(stop_words)

#stop word anti_join
tokenized <- tokenized %>%
  anti_join(stop_words)
```

```{r cleaning}
# remove all numbers
clean_tokens <- str_remove_all(tokenized$word, "[:digit:]") 

# remove 's contractions
clean_tokens <- gsub("’s", '', clean_tokens)

# remove 's contractions
clean_tokens <- gsub("'s", '', clean_tokens)

tokenized$clean <- clean_tokens

#remove the empty strings
tib <-subset(tokenized, clean!="")

#reassign
tokenized <- tib

#cleaned tokenized
tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 5) %>% 
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL) +
  labs(title = "Word Frequencies in Paragraphs")
```

4.  Recreate the publications per day and word frequency plots using the headlines variable (response.docs.headline.main). Compare the distributions of word frequencies between the first paragraph and headlines. Do you see any difference?

```{r date-plot2}
nytDat2 <- nyt_df

nytDat2 %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>% 
  group_by(pubDay) %>%
  arrange(pubDay) |> 
  summarise(count=n()) %>%
  filter(count >= 2) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") +
  coord_flip() #bring date so bars go lengthwise
```

```{r word_frequencies2}
#use tidytext::unnest_tokens to put in tidy form.  
#If there are some types of news that we'd like to exclude, we can filter()
tokenized_headline <- nytDat2 %>%
  filter(response.docs.news_desk!=c("Games","Sports")) %>%
unnest_tokens(word, response.docs.headline.main) #word is the new column, headline is the source

#stop word anti_join
tokenized_headline <- tokenized_headline %>%
  anti_join(stop_words)

# remove all numbers
clean_tokens_headline <- str_remove_all(tokenized_headline$word, "[:digit:]") 

# remove 's contractions
clean_tokens_headline <- gsub("’s", '', clean_tokens_headline)

# remove 's contractions
clean_tokens_headline <- gsub("'s", '', clean_tokens_headline)

tokenized_headline$clean_head <- clean_tokens_headline

#remove the empty strings
tib_headline <-subset(tokenized_headline, clean_head!="")

#reassign
tokenized_headline <- tib_headline

# cleaned tokenized_headline
tokenized_headline %>%
  count(clean_head, sort = TRUE) %>%
  filter(n > 5) %>% 
  mutate(clean_head = reorder(clean_head, n)) %>%
  ggplot(aes(n, clean_head)) +
  geom_col() +
  labs(y = NULL) +
  labs(title = "Word Frequencies in Headlines")
```

**There is a difference between the word frequency in the headlines and paragraphs however the words do tend to be similar, they primarily differ in order and count. The paragraphs word frequency has higher occurence counts primarily due to the fact that paragraphs are longer and use more words than headlines do. **
